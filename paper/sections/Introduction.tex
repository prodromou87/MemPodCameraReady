% !TEX root = ../HPCA2016.tex
\section{Introduction}
\label{sec:Introduction}

The memory wall problem has been known to impede system performance \cite{wulf-can95}. 3D stacked memory is considered as a viable solution and has been studied extensively. Placing memory stacks in the processor package was shown to provide significant improvements in terms of bandwidth and power consumption \cite{black-micro2013}. Processor manufacturers already began including 3D-stacked memories in their announced products \cite{KnightsLanding,NVIDIA,black-micro2013} and die-stacked memory standards have been developed \cite{jedec-wideio,JEDEC-HBM,pawlowski-hotchips2011}. The use of die-stacked memory will undeniably be part of many future systems. Products such as \TODO{Cite Fury-X, any other existing product (AMD?)} are already manufactured with stacks of memory in the same package as the processing cores.

The current technology for incorporating stacked memory, as well as the current protocols allow up to 16GB of
fast DRAM~\cite{KnightsLanding}. Commodity server systems often need {\em hundreds} of GB of memory. Consequently, at its current state, in-package memory cannot solely support today's memory requirements, leading to the emergence of \emph{hybrid memories}, usually with fast 3D-stacked memory and the traditionally used off-chip memory such as DDR4. Often, hybrid configurations with two memory technologies are called \emph{``Two-Level Memories''}, which implies a cache-like organization, even though that's not always the case. It is not clear yet how this newly added memory is best utilized. Recent research proposed mechanisms to manage the 3D-stacked memory as a high-bandwidth last level cache, while other proposals attempt to manage this memory as an extension of the main memory. Each approach comes with its own challenges, benefits and drawbacks. 

Recent proposals in the literature \cite{chou-micro2014,qureshi-micro2012} demonstrate that when stacked memory is used as a cache, we need to re-evaluate traditional SRAM cache optimizations. Tag placement and granularity must be carefully studied and double access (tag check and data retrieval) in order to serve one request must be avoided due to the high latency of accessing a DRAM structure. Despite the associated challenges, state-of-the-art proposals manage to achieve high performance improvement through elegant solutions. Such a management scheme offers transparent operation and does not require added hardware structures to support it. On the downside, the extra memory capacity is not available to the software and some space is wasted to store tags instead of useful information. As memory capacities increase, the size of tag information could result in wasting even larger parts of our fast memory.

Instead of using stacked-memory as cache, some recent research \cite{sim-micro2014,meswani-HPCA21} proposed both hardware and hardware-supported OS mechanisms to manage it as a \emph{``flat address space memory''}. Memory accesses are being monitored in hardware and the goal is to identify ``hot'' memory pages and migrate them into the fast memory, in order to improve performance. Such a configuration extends the exposed main memory capacity, serving capacity-constrained workloads better than a DRAM cache organization, while at the same time eliminating the issue of tag placement and retrieval. Additionally, a flat address space organization could be incorporated in a system without the presence of a memory manager and it would still operate correctly and boost performance simply because of the presence of a fast memory region. Like a DRAM cache, this memory organization is also transparent to the programmer. However, applications will occasionally need to be interrupted for the required migrations to take place. 

Flat address space memory managers need to overcome significant challenges. The main drawback comes from the required book-keeping. Monitoring memory regions and keeping track of migrated pages in order to relay incoming requests transparently, often comes at very high storage and power consumption overheads. Unlike a DRAM cache, there is no backing store memory. As such, each migration is a swap of two pages in order to ensure that exactly one copy of each participant page exists in main memory. In the presence of a flat memory with two regions, one fast and one slow, it could be expected that the fast one is fully utilized before we start using the slow one. However, modern OS's assign memory addresses randomly for security reasons \TODO{Find and cite} and as such no guarantees can be made for a ``smarter'' memory allocation policy.

Our goal in this paper is to design a memory manager for flat address space memory configurations that is efficient in terms of area requirements and performance, as well as scalable to future technology advancements in memory capacity and speed. MemPod is carefully designed to adhere to these goals. The contributions of this paper are:

\begin{itemize}
\item MemPod's novel microarchitectural design clusters existing memory controllers into memory ``Pods'' allowing for better scalability and integration to future systems with larger and faster memories.
\item Novel activity tracking algorithm: Based on a ``Majority Element Algorithm'' (MEA) heuristic designed for database management and big data analytics, MemPod achieves excellent future prediction at virtually no cost. To the best of our knowledge such an algorithm has never been used in this context before.
\item We provide a breakdown of the basic building blocks for \emph{any} flat address space migration mechanism. System designers can choose any solution for each of these elements and create a new management scheme in a plug-and-play fashion. Each building block is associated with its corresponding trade-off. Later in this paper we provide a description of each block, along with a side-by-side comparison of MemPod's approach and what the state-of-the-art mechanisms propose.
\end{itemize}

Our evaluation results under homogeneous and mixed 8-core multi-programmed workloads show MemPod to outperform the current state-of-the-art by 11\% on average and up to 29\% in terms of Average Main Memory Time. Under an overclocked memory configuration, our results show MemPod to be the most scalable mechanism as memory technology improves. The use of MEA activity tracking requires 10M times less space than traditionally used Full Counters (FC), while at the same time achieving 58\% better hot page identification (future prediction).

In Section \ref{sec:Background} of this paper we present background and related work regarding memory management schemes. Section \ref{sec:MEA} presents our novel activity tracking in detail along with an in-depth evaluation of its capabilities compared to traditionally used mechanisms. Section \ref{sec:Architecture} gives a detailed overview of MemPod's architecture, presented side-by-side with the state-of-the-art mechanisms we compare against. Our architecture section is organized based on the fundamental basic blocks of management mechanisms in order to present the management problem in its entirety. Section \ref{sec:Results} presents our experimental methodology and evaluation results and finally, Section \ref{sec:Conclusions} concludes the paper.