% !TEX root = ../MemPod.tex
\section{Introduction}
\label{sec:Introduction}

The memory wall problem has been known to impede system performance \cite{wulf-can95}. 3D stacked memory is considered as a viable solution and has been studied extensively. Placing memory stacks in the processor package was shown to provide significant improvements in terms of bandwidth and power consumption \cite{black-micro2013}. Processor manufacturers already began including 3D-stacked memories in their announced products \cite{KnightsLanding,NVIDIA,black-micro2013} and die-stacked memory standards have been developed \cite{jedec-wideio,JEDEC-HBM,pawlowski-hotchips2011}. The use of die-stacked memory will undeniably be part of many future systems. Products such as \TODO{Cite Fury-X, any other existing product (AMD?)} are already manufactured with stacks of memory in the same package as the processing cores.

\sout{The current technology for incorporating stacked memory, as well as the current protocols allow up to 16GB of
fast DRAM~\cite{KnightsLanding}.} \TODO{Find the current limits of HBM, mention them and cite.} Commodity server systems often need {\em hundreds} of GB of memory. Consequently, at its current state, in-package memory cannot solely support today's memory requirements, leading to the emergence of \emph{hybrid memories}, usually with fast 3D-stacked memory and the traditionally used off-chip memory such as DDR4. Often, hybrid configurations with two memory technologies are called \emph{``Two-Level Memories''}, \sout{which implies a cache-like organization, even though that's not always the case} or \emph{``N-Level Memories'' (NLM)} for configurations with N memory technologies. It is not clear yet how this newly added memory is best utilized. Recent research proposed mechanisms to manage the 3D-stacked memory as a high-bandwidth last level cache, while other proposals attempt to manage this memory as an extension of the main memory. Each approach comes with its own challenges, benefits and drawbacks. 

Recent proposals in the literature \cite{chou-micro2014,qureshi-micro2012} demonstrate that when stacked memory is used as a cache, we need to re-evaluate traditional SRAM cache optimizations. Tag placement and granularity must be carefully studied and double access (tag check and data retrieval) in order to serve one request must be avoided due to the high latency of accessing a DRAM structure. Despite the associated challenges, state-of-the-art proposals manage to achieve high performance improvement through elegant solutions. Such a management scheme offers transparent operation and does not require added hardware structures to support it. On the downside, the extra memory capacity is not available to the software and some space is wasted to store tags instead of useful information. \sout{As memory capacities increase, the size of tag information could result in wasting even larger parts of our fast memory.} In a DRAM cache configuration, tags consume \TODO{??\%} of the on-chip memory's capacity.

Instead of using stacked-memory as cache, some recent research \cite{sim-micro2014,meswani-HPCA21} proposed both hardware and hardware-supported OS dynamic memory management mechanisms to manage it as a \emph{``flat address space memory''}. Memory accesses are being monitored in hardware and the goal is to identify ``hot'' memory pages and migrate them into the fast memory, in order to improve performance. Such a configuration extends the exposed main memory capacity, serving capacity-constrained workloads better than a DRAM cache organization, while at the same time eliminating the issue of tag placement and retrieval. Additionally, a flat address space organization could be incorporated in a system without the presence of a dynamic memory management mechanism and it would still operate correctly and boost performance simply because of the presence of a fast memory region and static memory allocation from the OS. Like a DRAM cache, this memory organization is also transparent to the application programmer. However, applications will occasionally need to be interrupted for the required migrations to take place. 

Flat address space dynamic memory managers need to overcome significant challenges. The main drawback comes from the required book-keeping. Monitoring memory regions and keeping track of migrated pages in order to relay incoming requests transparently, often comes at very high storage and power consumption overheads. Unlike a DRAM cache, there is no backing store memory. As such, each migration is a swap of two pages in order to ensure that exactly one copy of each participant page exists in main memory. In the presence of a flat memory with two regions, one fast and one slow, it could be expected that the fast one is fully utilized before we start using the slow one. However, modern OS's assign memory addresses randomly for security reasons \TODO{Find and cite} and as such no guarantees can be made for a ``smarter'' memory allocation policy.

In this paper, we propose MemPod, a dynamic memory manager for flat address space memory configurations that is efficient in terms of area requirements and performance, as well as scalable to future technology advancements in memory capacity and speed. MemPod's novel microarchitectural design clusters existing memory controllers into memory ``Pods" allowing for better scalability and integration to future systems with larger and faster memories. For MemPod's activity tracking requirements we incorporated a \emph{``Majority Element Algorithm'' (MEA)} heuristic, originally proposed for database management and big data analytics. Our evaluation shows MEA to be capable of high prediction accuracy with very low hardware overhead. To the best of our knowledge, such an algorithm has not been utilized before for activity tracking.

Our evaluation results under homogeneous and mixed 8-core multi-programmed workloads show MemPod to outperform the current state-of-the-art by 11\% on average and up to 29\% in terms of Average Main Memory Time (i.e. the average time a request spends in main memory). Under an overclocked memory configuration, our results show MemPod to be the most scalable mechanism as memory technology improves. The use of MEA activity tracking requires $\sim0.01\%$ of the storage space required by \sout{traditionally used} Full Counters (FC) used in previous research studies, while at the same time achieving 58\% higher \sout{hot page identification (future prediction)} prediction accuracy.

A goal of this paper is to identify the basic building blocks of \emph{any} flat address space dynamic memory management mechanism. System designers can choose any solution for each of these elements and create a new management scheme in a plug-and-play fashion. Each building block is associated with its corresponding trade-off. Later in this paper we provide a description of each block, along with a side-by-side comparison of MemPod's approach and what the state-of-the-art mechanisms propose.

The contributions of this paper are:

\begin{itemize}
\item Novel microarchitectural clustered design.
\item Novel activity tracking algorithm.
\item Breakdown of the basic building blocks of \emph{any} flat address space dynamic memory management mechanism. 
\end{itemize}

In Section \ref{sec:Background} of this paper we present background and related work regarding memory management schemes. Section \ref{sec:MEA} presents our novel activity tracking in detail along with an in-depth evaluation of its capabilities compared to traditionally used mechanisms. Section \ref{sec:Architecture} gives a detailed overview of MemPod's architecture, presented side-by-side with the state-of-the-art mechanisms we compare against. Our architecture section is organized based on the fundamental basic blocks of management mechanisms in order to present the management problem in its entirety. Section \ref{sec:Results} presents our experimental methodology and evaluation results and finally, Section \ref{sec:Conclusions} concludes the paper.