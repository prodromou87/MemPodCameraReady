% !TEX root = ../MemPod.tex

\section{Related Work}
\label{sec:Background}

A wide range of research proposals studied and developed new technologies in order to overcome the memory wall. 
%% NUWAN: In the interest of space, I suggest dropping the memory controller scheduling discussion as it's not directly relevant
%%Improving the performance of Main Memory can be achieved via optimized memory scheduling and optimized page placement, introducing new technologies such as stacked high-bandwidth memory and using it alongside off-chip memories in a hybrid configuration, as well as mechanisms that manage the hybrid configuration as a DRAM cache or a flat address space.
%%
%%Several memory scheduling proposals attempt to improve performance and bandwidth of a single Memory Controller (MC) in isolation \cite{TCM,PARBS,SMS} or orchestrate the operation of all MCs in a system in order to improve the overall system's performance \cite{ATLAS}. Memory schedulers build upon the observation that rearranging memory requests before sending them to memory can often lead to higher memory parallelism and significant performance improvement. Prioritizing some requests over others can lead to starvation, which is one of the main challenges in memory scheduling research. 
Techniques such as \cite{BUMP,RMM,SUPERPAGES} attempt to optimize page placement in memory in order to expose higher parallelism. However, these scheduling mechanisms do not take advantage of a faster memory in a hybrid configuration.

Stacking DRAM dies in the processor package has been shown to achieve significant performance improvement. Being in its infancy, this technology cannot yet deliver large capacities \cite{JEDEC-HBM-REVISED}. Consequently, configurations with stacked and off-chip memories have been proposed \cite{LOH-HYBRID,qureshi-micro2012} and can be found in the literature as ``hybrid memories'' or ``Two-level memories''. The community has been trying to identify the optimal organization for this new technology, mainly using it as a large high-bandwidth last level cache or as a ``flat address space'', where the capacity of the stacked memory is exposed to the software.

Organizing stacked memory as a cache has been explored in several studies \cite{qureshi-micro2012,BEAR,BIMODAL,citadel,tagless-dram-cache,UNISON}. These approaches need to implement intelligent tag stores in order to allow cache-like operation while mitigating the penalties introduced by reading tags. It has been demonstrated that traditional SRAM cache optimizations result in degraded performance when used in a DRAM cache and as such we need to ``de-optimize for performance'' \cite{qureshi-micro2012}. DRAM cache organizations have been shown to improve performance significantly in latency-limited applications, while offering only marginal improvement with capacity-limited applications. It's been shown that exposing the extra capacity to the application instead of using it as a cache can benefit capacity-limited applications. To this end, recent work \cite{meswani-HPCA21,sim-micro2014,cameo} proposed mechanisms to manage stacked memory as a flat address space. 

HMA \cite{meswani-HPCA21} was proposed as a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support was required for profiling memory accesses using counters for each memory page, while the migration part was handled by the OS. Due to the costly OS involvement, HMA's intervals had to be kept large and the hardware cost of its profiling counters was significant. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.

Sim et al. proposed a technique for transparent hardware management of a hybrid memory system \cite{sim-micro2014}, which we will refer to as ``THM'' throughout this paper as the authors did not propose a name for their mechanism. THM does not require any OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one ``competing counter'' per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.

CAMEO \cite{cameo} operates similarly to THM and restricts migrations within segments with one fast page location per segment, but operates at a smaller granularity referred to as a ``line''. Its bookkeeping structures are entirely stored in memory, while a ``Line Location Predictor'' attempts to save some bookkeeping-related accesses by predicting the location of a line. CAMEO initiates a line migration upon every access to slow memory, serving as a cache-like flat address space configuration.

Both THM and CAMEO sacrifice migration potential for area efficiency by restricting migrations in segments. If more than one hot pages exist within the same segment only one can reside in fast memory. Similarly, if no hot pages exist in a segment, its fast page cannot be utilized by some other segment. THM's competing counters can lead to false positives, allowing a cold page to migrate to fast memory, while CAMEO's migrations upon each access can result in high migration traffic.