% !TEX root = ../MemPod.tex

\section{Related Work}
\label{sec:Background}

A wide range of research proposals have sought to address the memory wall. 
%% NUWAN: In the interest of space, I suggest dropping the memory controller scheduling discussion as it's not directly relevant
%%Improving the performance of Main Memory can be achieved via optimized memory scheduling and optimized page placement, introducing new technologies such as stacked high-bandwidth memory and using it alongside off-chip memories in a hybrid configuration, as well as mechanisms that manage the hybrid configuration as a DRAM cache or a flat address space.
%%
%%Several memory scheduling proposals attempt to improve performance and bandwidth of a single Memory Controller (MC) in isolation \cite{TCM,PARBS,SMS} or orchestrate the operation of all MCs in a system in order to improve the overall system's performance \cite{ATLAS}. Memory schedulers build upon the observation that rearranging memory requests before sending them to memory can often lead to higher memory parallelism and significant performance improvement. Prioritizing some requests over others can lead to starvation, which is one of the main challenges in memory scheduling research. 
Techniques such as \textit{Bump\cite{BUMP}, RMM\cite{RMM} and Superpages\cite{SUPERPAGES}} attempt to optimize page placement in memory in order to expose higher parallelism. However, these scheduling mechanisms do not take advantage of a faster memory in a hybrid configuration.

Stacking DRAM dies in the processor package has been shown to achieve significant performance improvement. This technology cannot yet deliver large capacities \cite{JEDEC-HBM-REVISED}. Consequently, configurations that combine
stacked and off-chip memories have been proposed~\cite{LOH-HYBRID,qureshi-micro2012} and can be found in the literature as ``hybrid memories'' or ``two-level memories''. The systems have proposed the use of the stacked memory
either as a large high-bandwidth last level cache or as a ``flat address space'', where the capacity of the stacked memory is exposed to the software.

Organizing stacked memory as a cache has been explored in several studies \cite{qureshi-micro2012,BEAR,BIMODAL,citadel,tagless-dram-cache,UNISON}. These approaches implement intelligent tag stores to allow cache-like operation while
 mitigating the cost of reading tags in DRAM. It has been demonstrated that traditional SRAM-tailored cache optimizations result in degraded performance when used in a DRAM cache and as such we need to ``de-optimize for performance'' \cite{qureshi-micro2012}. \remark{Nuwan's comment on next sentence: While the statement is true, do we buy that claim (as at least the current stacked memories don't reduce latency)? I.e., should we drop this sentence?\\A.P.: The truth is most of the DRAM cache mechanisms used a very fast stacked memory (3.2GHz) with a slow off-chip one which should definitely help their results. The JEDEC HBM standard does not specify frequencies.} DRAM cache organizations have been shown to improve performance significantly in latency-limited applications, while offering only marginal improvement with capacity-limited applications. It's been shown that exposing the extra capacity to the application instead of using it as a cache can benefit capacity-limited applications. To this end, recent work~\cite{meswani-HPCA21,sim-micro2014,cameo} proposes mechanisms to manage stacked memory as a flat address space. 

\remark{try to avoid using past tense to refer to other papers.  Makes them
sound old and obsolete.}

HMA \cite{meswani-HPCA21} is a HW/SW mechanism that attempts to predict frequently accessed pages in memory and, at predefined intervals, migrate those pages to fast memory. HW support is required for profiling memory accesses using counters for each memory page, while the migration is handled by the OS. Due to the costly OS involvement, HMA's intervals are kept large.  Additionally, 
the hardware cost of its profiling counters is high. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages as the OS can update page tables and TLBs to reflect migrations.

Sim et al. proposed a technique for transparent hardware management of a hybrid memory system \cite{sim-micro2014}, which we will refer to as ``THM''.
THM does not require any OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page and a set of slow memory pages. The slow pages of each segment can only migrate to the one fast page location, and any such migration results in the eviction of the currently-residing page. THM monitors memory accesses with one ``competing counter'' per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.

CAMEO \cite{cameo} proposes a cache-like flat address space memory management scheme in an attempt to close the gap between cache and flat memory organizations. CAMEO operates similarly to THM, however it does so at the granularity of cache lines (64B). Migrations are restricted within segments with one fast line location per segment. Its bookkeeping structures are entirely stored in memory, while a ``Line Location Predictor'' attempts to save some bookkeeping-related accesses by predicting the location of a line. CAMEO initiates a line migration upon every access to slow memory.

Both THM and CAMEO sacrifice migration flexibility for area efficiency by restricting migrations in segments: if more than one hot page/line exists within the same segment only one can reside in fast memory. If no hot pages exist in a segment, its fast page cannot be utilized by another segment. Further, THM's competing counters can lead to false positives, allowing a cold page to migrate to fast memory, while CAMEO can incur high migration traffic as every access could induce a migration. 

Spatial locality of applications can affect performance negatively when THM or CAMEO are used. Continuous pages or lines that lie within the same segment of each mechanism can be accessed frequently. THM is less susceptible to such issues because of its coarser granularity and the use of competing counters that will prevent a ``ping-pong'' effect. CAMEO however is significantly affected. This issue is further exacerbated when the ratio between slow and fast memory capacities is increased. In such scenarios, under a configuration with 1:8 fast:slow memory ratio, both mechanisms suffer from reduced migration flexibility,
e.g. forcing 8 slow pages to fight over a {\em single} fast page or
line.  In CAMEO's case, since every access to a slow line triggers a migration,
a high slow-fast capacity ratio can result in the majority of accesses going
to slow lines, causing a migration in every case.
