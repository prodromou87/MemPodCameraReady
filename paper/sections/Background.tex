% !TEX root = ../MemPod.tex

\section{Related Work}
\label{sec:Background}

A wide range of research proposals studied and developed new technologies in order to overcome the memory wall obstacle. Improving the performance of Main Memory can be achieved via optimized memory scheduling and optimized page placement, introducing new technologies such as stacked high-bandwidth memory and using it alongside off-chip memories in a hybrid configuration, as well as mechanisms that manage the hybrid configuration as a DRAM cache or a flat address space.

Several memory scheduling proposals attempt to improve performance and bandwidth of a single Memory Controller (MC) in isolation \cite{TCM,PARBS,SMS} or orchestrate the operation of all MCs in a system in order to improve the overall system's performance \cite{ATLAS}. Memory schedulers build upon the observation that rearranging memory requests before sending them to memory can often lead to higher memory parallelism and significant performance improvement. Prioritizing some requests over others can lead to starvation, which is one of the main challenges in memory scheduling research. Other proposals such as [BUMP, Redundant Memory Mappings, Superpages]\cite{BUMP,RMM,SUPERPAGES} attempt to optimize page placement in memory in order to expose higher parallelism. At their current state, scheduling mechanisms do not take advantage of a faster memory in a hybrid configuration since they are not responsible for moving memory pages from their initial location.

Stacking DRAM dies in the processor package has been shown to achieve significant performance improvement. Being at its infancy, this technology cannot yet deliver large capacities\cite{JEDEC-HBM-REVISED}. Consequently, configurations with stacked and off-chip memories have been proposed \cite{LOH-HYBRID,qureshi-micro2012} and can be found in the literature as ``hybrid memories'' or ``Two-level memories''. The community has been trying to identify the optimal organization for this new technology, mainly using it as a large high-bandwidth last level cache or as a ``flat address space'', where the capacity of the stacked memory is added to the main memory capacity and exposed to the running programs.

Organizing stacked memory as a cache has been studied in \cite{qureshi-micro2012,BEAR,BIMODAL,citadel,tagless-dram-cache,UNISON}. These mechanisms need to implement intelligent tag stores in order to allow cache-like operation, while at the same time mitigating the introduced penalties from reading these tags before reading data from memory. It has been demonstrated that traditional SRAM cache optimizations result in degraded performance when used in a DRAM cache and as such we need to ``de-optimize for performance'' \cite{qureshi-micro2012}. DRAM cache organizations have been shown to improve performance significantly in latency-limited applications, while offering only marginal improvement with capacity-limited applications. It's been shown that exposing the extra capacity to the application instead of using it as a cache can benefit capacity-limited applications. To this end, recent work \cite{meswani-HPCA21,sim-micro2014,cameo} proposed mechanisms to manage stacked memory as a flat address space. 

HMA was proposed in \cite{meswani-HPCA21} as a HW/SW mechanism that attempts to predict frequently accessed pages in memory and -- upon predefined intervals -- migrate those pages in the fast memory. HW support was required for profiling memory accesses using counters for each memory page, while the migration part was handled by the OS. Due to the costly OS involvement, HMA's intervals had to be kept large and the hardware cost of its profiling counters was significant. However, HMA is capable of managing migrations in a flat address space without the need of additional bookkeeping for finding migrated pages, since the OS can update Page Tables and TLBs and keep everything transparent.

THM was proposed in \cite{sim-micro2014}. We use ``THM'' as a shorthand throughout this paper since the authors did not propose a name for their mechanism. THM does not require any OS intervention while managing migrations. In order to keep bookkeeping costs manageable, THM allows migrations only within sets of pages (called segments). Each segment includes one fast memory page while the rest are stored in slow memory. The pages of each segment can only migrate to the one fast page location resulting in the eviction of the currently-residing page. THM monitors memory accesses with one ``competing counter'' per segment resulting in a low cost profiling solution. Finally, THM supports caching part of its structures on chip while the rest is stored in memory.

CAMEO \cite{cameo} operates similarly to THM and restricts migrations within segments with one fast page location per segment, but operates at a smaller granularity (called a line in the published paper). Its bookkeeping structures are entirely stored in memory, while a ``Line Location Predictor'' attempts to save some bookkeeping-related accesses by predicting the location of a line. CAMEO initiates a line migration upon every access to slow memory, serving as a cache-like flat address space configuration.

Both THM and CAMEO sacrifice migration potential for area efficiency by restricting migrations in segments. If more than one hot pages exist within the same segment only one can reside in fast memory and similarly, if no hot pages exist in a segment, its fast page cannot be utilized by some other segment. THM's competing counters can lead to false positives, allowing a cold page to migrate to fast memory, while CAMEO's migrations upon each access can result in high migration traffic.