% !TEX root = ../HPCA2016.tex
\section{Results}
\label{sec:Results}

\subsection{Evaluation Framework}
\label{sub:Evaluation}

The goal of our evaluation framework is to quantitatively and qualitatively assess MemPod's capabilities and compare it against state-of-the-art proposed mechanisms. Throughout our evaluation section, we study MemPod's performance running as part of an eight-core CPU. We extended Ramulator \cite{kim-ramulator} to support flat address space hybrid memories and with MemPod for our memory simulations. HMA and THM were also implemented in our simulation framework for comparison purposes. Ramulator allows for cycle-accurate memory simulation and includes a simple CPU front-end capable of approximating resource-induced stalls. We chose to evaluate MemPod under a realistic memory configuration consisting of 1GB 3D-stacked HBM2.0 \TODO{[Cite]} and 8GB of off-chip DDR4-1600. Table \ref{tab:specs} Provides a more detailed description of the simulated system's configuration.

\subsection{Experimental Methodology}
\label{sub:Experimental}

We used benchmarks from the SPEC2006 suite \cite{spec} as our workloads. Using Sniper \cite{sniper}, we extracted memory request traces while simultaneously executing 8 benchmarks on a simulated 8-core CPU. We then feed these multi-programmed memory traces in Ramulator, executing all workloads to completion. Our complete set of workloads consists of 16 ``homogeneous'' workloads, where the same benchmark runs 8 times in parallel (we simply call this workload with the benchmark's name in later results), as well as 12 workloads featuring a random mix of 8 benchmarks each (marked as mix1-12). A breakdown of the mixed workloads is shown in Table \ref{tab:workloads}.

We also extended Ramulator with the ability to simulate cache memories with each mechanism to allow a realistic evaluation. When caches are disabled, the simulator assumes that any information needed by any mechanism exists on chip and is accessible without any delay. The migration process was implemented in detail as well. In order to read an entire page from memory, 32 read requests need to be sent and then another set of 32 requests will write the page back in memory.

Due to space limitation we are not able to show results from all our workloads in most of the graphs in this paper. In those graphs, we only present the results from mixed workloads, the average of all mixed workloads, average of all homogeneous workloads and overall average.

\begin{table}[t]
  \includegraphics[width=0.46\textwidth]{figures/specs_table.pdf}
  \caption{Experimental framework configuration}
  \label{tab:specs}
\end{table}

\begin{table}[t]
  \includegraphics[width=0.46\textwidth]{figures/workloads_checkmarks.pdf}
  \caption{Mixed workloads description}
  \label{tab:workloads}
\end{table}
%
%\begin{table}
%  \includegraphics[width=0.46\textwidth]{figures/workload_characterization.pdf}
%  \caption{Experimental framework configuration}
%  \label{fig:specs}
%\end{table}

\subsection{Simulation Results}
\label{sub:SimResults}

\subsubsection{Optimal Parameter Values}

\begin{figure*}[t]
	\centering
  \includegraphics[width=\textwidth]{figures/num_counters_normalized.pdf}
  \caption{\# of MEA Counters Vs Normalized AMMT (primary axis) and average \# of Migrations per Pod per interval (secondary axis)}
  \label{fig:num_counters}
\end{figure*}


\begin{figure*}[t]
  \includegraphics[width=\textwidth]{figures/interval_length_normalized.pdf}
  \caption{Interval Length Vs Normalized AMMT (primary axis) and average \# of Migrations per Pod per interval (secondary axis)}
  \label{fig:interval}
\end{figure*}

MemPod exposes 3 variables that allow fine-tuning it based on a particular system or expected workloads: (1) The number of MEA counters, (2) interval length and (3) the size of each MEA counter. Identifying the optimal values for each parameter can maximize MemPod's capabilities. The number of MEA counters dictates the highest possible number of migrations that can be performed at each interval, while the epoch length will determine MemPod's ability to better adapt to phase changes in a workload. The size of each MEA counter can affect performance due to the loss of information when smaller counters are used but can also save space on the chip.

We first identified the optimal number of MEA counters, by setting the epoch length to 500us and exponentially increasing the number of counters from 16 to 512. In order to minimize the impact of other factors, we executed this experiment with 16 bits per counter and caches disabled. In other words, each counter was given more than enough space and all required information such as the remap table resides entirely on the chip and is accessible at no cost. 

Figure \ref{fig:num_counters} shows normalize Average Main Memory Time (AMMT)\footnote{AMMT: The average time a request spends in main memory}, along with the average number of migrations per Pod per epoch (secondary axis). The results indicate that each Pod utilizes the higher number of counters and consecutively performs more migrations per interval, however performance begins leveling off when more than 128 counters were used. More migrations can directly be translated into higher power consumption and communication cost. Using 128 counter, MemPod improves AMMT by 7\% over the baseline with 16 MEA counters. Based on our observations we conclude that the optimal value for this parameter is 128 and will be used for the remainder of this section.

After identifying the optimal counter number, Figure \ref{fig:interval} displays the same measurements as Figure \ref{fig:num_counters} with a varying epoch length. Caches were again disabled, each counter was given 16 bits and the number of MEA counters was set to the optimal value of 128. In most of our benchmarks a smaller epoch length leads to higher performance. In accordance to the previous experiment, higher epoch lengths also lead to higher number of migrations. On average, an epoch length of 100us outperforms the baseline (50us) by 2\%. For comparison purposes, HMA ?? identified the optimal epoch length to be 1ms (10x larger) in order to support all lengthy processes that take place during a migration event.

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{figures/counter_size_normalized.pdf}
  \caption{Counter size (in bits) Vs Normalized AMMT (primary axis) and average \# of Migrations per Pod per interval (secondary axis)}
  \label{fig:counter_size}
\end{figure*}

As previously explained in Section ??, the MEA algorithm cannot be cached efficiently and as a result, the entire activity tracking structure needs to be on chip. The size (in bits) of each counter defines the area requirements of our MEA tracking mechanism. We modified our MEA counters to overflow to the value of 1 instead of 0. We opted not to immediately remove an overflowed counter since the existence of the correct counter set is more crucial to the algorithm's accuracy than our proposed modification. For this experiment we used the optimal parameter values identified in the previous experiments and set the number of MEA counters to 128 and interval length to 100us. All caching was disable in order to study the direct impact of this variable.

Figure \ref{fig:counter_size} presents the impact of counter size on AMMT and average number of migrations. We first observe that 8 bits are sufficient for the majority of our workloads, since larger sizes report identical results. Our second observation is that two bit counters report a negligible performance degradation (0.3\% on average) and a reduced average number of migrations.

The most interesting observation comes from assigning 4 bits to each counter. On average, performance is boosted slightly (up to 3\% and 1\% on average) even compared to using larger counters, while migration count is smaller. Since a difference is observed, we can conclude that these smaller counters ``lose'' information that in turn benefits overall execution. The reported result is a welcomed artifact of the MEA algorithm combined with application behavior. 

Based on our results, we identify 4 bits per counter to be the optimal value leading to an area cost of only $\sim$1KB per Pod and $\sim$4KB total. Compared to the state of the art, MemPod's activity tracking requirement is $\sim$128x smaller than THM's (512KB) and $\sim$2300x smaller than HMA's (9MB).


\subsubsection{Performance Comparison}
\label{sub:performance}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{figures/performance_over_nlm.pdf}
  \caption{Performance Comparison: AMMT is normalized to a hybrid memory without any migration mechanism.}
  \label{fig:performance}
\end{figure*}

Figure \ref{fig:performance} presents a performance comparison of MemPod, HMA, THM and a configuration with no migration capabilities (NLM). We evaluated all mechanisms with caching disabled. 

Based on the results we can derive some interesting observations:
\begin{itemize}
	\item In some workloads migration is harmful to performance, as observed with the gems workload, where a no-migration scheme reports higher performance (lower AMMT). We observe that in those cases, MemPod leads to deteriorated performance compared to THM. However, in the case of zeusmp, MemPod increases performance, while THM and HMA report higher AMMT.
	\item MemPod outperforms the state-of-the-art competitors in the majority of our workloads, and in several cases scoring very close to an HBM2-only configuration. 
	\item On average MemPod reports 25\% higher AMMT than HBM2-only, while THM and HMA report 39\% and 41\% respectively.
	\item All mechanisms outperform HBM2-only when executing the libquantum experiment. We attribute this result to a combination of correct timing, application behavior and workload size. In the case of libquantum, to working set size fits entirely in our fast memory. As a results after some migrations, the entire working set will be present in our HBM. However, correct timing is the driving factor behind this impressive performance increase. Our results show the row-buffer hit ratio to be ??x times larger than having HBM2-only (and random page assignment). Apparently, page migrations resulted in an in-memory page order that exploits almost every bit of memory parallelism from the application. This result could be further explored and intentionally recreated in some future work.
\end{itemize}

\subsubsection{Caching Effect}

\TODO{I need to run one small experiment to complete this subsection. We also need to finalize how we want to model HMA's interrupt, PT updates and cold TLBs. Based on previous discussions, we can assume that sorting HMA's counters is completely overlapped by servicing requests (in the best case) and as such HMA doesn't stall for sorting.}

Migration mechanisms will be forced to include a cache since activity tracking and remap table structures are commonly too large to hold on-chip. The use of a cache will unavoidably hinder performance. In this experiment we evaluate the impact of a cache on each mechanism's performance. As described in Section \ref{sec:Architecture}, each mechanism has different cache requirements. THM caches its counters and remap table together with its ``SRT'' structure. HMA has no need for a remap table however it has high storage requirements for its counting mechanism. MemPod only needs to cache its large remap table since MEA counters will be on chip. For the purposes of this experiment, we simulate all mechanisms with a 64kB direct-mapped cache. For MemPod, cache is divided equally over four Pods (16kB per Pod).

HMA's design further complicates this study, since sorting all activity counters at each epoch is performed by the OS, utilizing the cpu's cache instead of the dedicated migration cache. Our HMA results do not include any penalties related to sorting, OS interrupts and cold TLBs and Page Tables. As such, the reported HMA values can be considered overly optimistic.

Part of our study was to determine if the slow off-chip memory could be an acceptable solution to serve as the backing store location of each mechanism's structures. If the reported performance is acceptable, using the DDR4 memory would be ideal since we won't be reducing the effective capacity of the small HBM.

Figure ?? shows each mechanism's relative slowdown, normalized to the performance of the corresponding mechanism when no cache is simulated. \TODO{DISCUSS}

